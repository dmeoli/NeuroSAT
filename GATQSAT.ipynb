{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "name": "GATQSAT.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmeoli/NeuroSAT/blob/master/GATQSAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "md4JZ9dNr-cj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e115cb1-8c0b-4d1c-e090-2a1c2f5fcffa"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "id": "md4JZ9dNr-cj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSSjiEf1sjTf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d098f9fb-171e-4596-f77f-c3c7e0505bfd"
      },
      "source": [
        "%cd gdrive/My Drive"
      ],
      "id": "PSSjiEf1sjTf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQDmH68WssAm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6ec07ff-83ed-4add-8e3e-0dd0128f5391"
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir('neuroSAT'):\n",
        "  !git clone --recurse-submodules https://github.com/dmeoli/neuroSAT\n",
        "  %cd neuroSAT\n",
        "else:\n",
        "  %cd neuroSAT\n",
        "  !git pull"
      ],
      "id": "wQDmH68WssAm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/neuroSAT\n",
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDLLNcrEt_il",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f76b67a8-351c-4d9e-de72-b000fc4f37c8"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "id": "FDLLNcrEt_il",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/cu113/torch_stable.html, https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
            "Collecting gym==0.20.0\n",
            "  Downloading gym-0.20.0.tar.gz (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.19.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (3.13)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.4.1)\n",
            "Collecting split-folders\n",
            "  Downloading split_folders-0.4.3-py3-none-any.whl (7.4 kB)\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.4.1-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 91.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (2.7.0)\n",
            "Collecting torch==1.10.0+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torch-1.10.0%2Bcu113-cp37-cp37m-linux_x86_64.whl (1821.5 MB)\n",
            "\u001b[K     |██████████████▋                 | 834.1 MB 1.3 MB/s eta 0:12:50tcmalloc: large alloc 1147494400 bytes == 0x5611c38c8000 @  0x7f55de640615 0x5611c00884cc 0x5611c016847a 0x5611c008b2ed 0x5611c017ce1d 0x5611c00fee99 0x5611c00f99ee 0x5611c008cbda 0x5611c00fed00 0x5611c00f99ee 0x5611c008cbda 0x5611c00fb737 0x5611c017dc66 0x5611c00fadaf 0x5611c017dc66 0x5611c00fadaf 0x5611c017dc66 0x5611c00fadaf 0x5611c008d039 0x5611c00d0409 0x5611c008bc52 0x5611c00fec25 0x5611c00f99ee 0x5611c008cbda 0x5611c00fb737 0x5611c00f99ee 0x5611c008cbda 0x5611c00fa915 0x5611c008cafa 0x5611c00fac0d 0x5611c00f99ee\n",
            "\u001b[K     |██████████████████▌             | 1055.7 MB 1.2 MB/s eta 0:10:34tcmalloc: large alloc 1434370048 bytes == 0x561207f1e000 @  0x7f55de640615 0x5611c00884cc 0x5611c016847a 0x5611c008b2ed 0x5611c017ce1d 0x5611c00fee99 0x5611c00f99ee 0x5611c008cbda 0x5611c00fed00 0x5611c00f99ee 0x5611c008cbda 0x5611c00fb737 0x5611c017dc66 0x5611c00fadaf 0x5611c017dc66 0x5611c00fadaf 0x5611c017dc66 0x5611c00fadaf 0x5611c008d039 0x5611c00d0409 0x5611c008bc52 0x5611c00fec25 0x5611c00f99ee 0x5611c008cbda 0x5611c00fb737 0x5611c00f99ee 0x5611c008cbda 0x5611c00fa915 0x5611c008cafa 0x5611c00fac0d 0x5611c00f99ee\n",
            "\u001b[K     |███████████████████████▌        | 1336.2 MB 1.3 MB/s eta 0:06:26tcmalloc: large alloc 1792966656 bytes == 0x56125d70a000 @  0x7f55de640615 0x5611c00884cc 0x5611c016847a 0x5611c008b2ed 0x5611c017ce1d 0x5611c00fee99 0x5611c00f99ee 0x5611c008cbda 0x5611c00fed00 0x5611c00f99ee 0x5611c008cbda 0x5611c00fb737 0x5611c017dc66 0x5611c00fadaf 0x5611c017dc66 0x5611c00fadaf 0x5611c017dc66 0x5611c00fadaf 0x5611c008d039 0x5611c00d0409 0x5611c008bc52 0x5611c00fec25 0x5611c00f99ee 0x5611c008cbda 0x5611c00fb737 0x5611c00f99ee 0x5611c008cbda 0x5611c00fa915 0x5611c008cafa 0x5611c00fac0d 0x5611c00f99ee\n",
            "\u001b[K     |█████████████████████████████▊  | 1691.1 MB 1.2 MB/s eta 0:01:52tcmalloc: large alloc 2241208320 bytes == 0x5611c38c8000 @  0x7f55de640615 0x5611c00884cc 0x5611c016847a 0x5611c008b2ed 0x5611c017ce1d 0x5611c00fee99 0x5611c00f99ee 0x5611c008cbda 0x5611c00fed00 0x5611c00f99ee 0x5611c008cbda 0x5611c00fb737 0x5611c017dc66 0x5611c00fadaf 0x5611c017dc66 0x5611c00fadaf 0x5611c017dc66 0x5611c00fadaf 0x5611c008d039 0x5611c00d0409 0x5611c008bc52 0x5611c00fec25 0x5611c00f99ee 0x5611c008cbda 0x5611c00fb737 0x5611c00f99ee 0x5611c008cbda 0x5611c00fa915 0x5611c008cafa 0x5611c00fac0d 0x5611c00f99ee\n",
            "\u001b[K     |████████████████████████████████| 1821.5 MB 1.1 MB/s eta 0:00:01tcmalloc: large alloc 1821458432 bytes == 0x56124922a000 @  0x7f55de63f1e7 0x5611c00be067 0x5611c00884cc 0x5611c016847a 0x5611c008b2ed 0x5611c017ce1d 0x5611c00fee99 0x5611c00f99ee 0x5611c008cbda 0x5611c00fac0d 0x5611c00f99ee 0x5611c008cbda 0x5611c00fac0d 0x5611c00f99ee 0x5611c008cbda 0x5611c00fac0d 0x5611c00f99ee 0x5611c008cbda 0x5611c00fac0d 0x5611c00f99ee 0x5611c008cbda 0x5611c00fac0d 0x5611c008cafa 0x5611c00fac0d 0x5611c00f99ee 0x5611c008cbda 0x5611c00fb737 0x5611c00f99ee 0x5611c008cbda 0x5611c00fb737 0x5611c00f99ee\n",
            "tcmalloc: large alloc 2276827136 bytes == 0x5612b5b3e000 @  0x7f55de640615 0x5611c00884cc 0x5611c016847a 0x5611c008b2ed 0x5611c017ce1d 0x5611c00fee99 0x5611c00f99ee 0x5611c008cbda 0x5611c00fac0d 0x5611c00f99ee 0x5611c008cbda 0x5611c00fac0d 0x5611c00f99ee 0x5611c008cbda 0x5611c00fac0d 0x5611c00f99ee 0x5611c008cbda 0x5611c00fac0d 0x5611c00f99ee 0x5611c008cbda 0x5611c00fac0d 0x5611c008cafa 0x5611c00fac0d 0x5611c00f99ee 0x5611c008cbda 0x5611c00fb737 0x5611c00f99ee 0x5611c008cbda 0x5611c00fb737 0x5611c00f99ee 0x5611c008d271\n",
            "\u001b[K     |████████████████████████████████| 1821.5 MB 5.4 kB/s \n",
            "\u001b[?25hCollecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_sparse-0.6.12-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 32.6 MB/s \n",
            "\u001b[?25hCollecting torch-geometric\n",
            "  Downloading torch_geometric-2.0.2.tar.gz (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 95.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.20.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0+cu113->-r requirements.txt (line 9)) (3.10.0.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX->-r requirements.txt (line 6)) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX->-r requirements.txt (line 6)) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (1.1.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (12.0.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (0.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (1.13.3)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (2.7.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (0.22.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (2.7.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (1.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (1.6.3)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (0.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (3.1.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (2.7.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (3.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (0.37.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (1.42.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->-r requirements.txt (line 7)) (1.5.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (3.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric->-r requirements.txt (line 13)) (4.62.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric->-r requirements.txt (line 13)) (2.6.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric->-r requirements.txt (line 13)) (1.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric->-r requirements.txt (line 13)) (1.1.5)\n",
            "Collecting rdflib\n",
            "  Downloading rdflib-6.0.2-py3-none-any.whl (407 kB)\n",
            "\u001b[K     |████████████████████████████████| 407 kB 87.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric->-r requirements.txt (line 13)) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric->-r requirements.txt (line 13)) (2.11.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric->-r requirements.txt (line 13)) (3.0.6)\n",
            "Collecting yacs\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric->-r requirements.txt (line 13)) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric->-r requirements.txt (line 13)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric->-r requirements.txt (line 13)) (2018.9)\n",
            "Collecting isodate\n",
            "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric->-r requirements.txt (line 13)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric->-r requirements.txt (line 13)) (3.0.0)\n",
            "Building wheels for collected packages: gym, torch-geometric\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.20.0-py3-none-any.whl size=1650477 sha256=00a78518ef4b5a2b6faa291d3ae52b2bcf41792b557e42c22b18689b8fbb06d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/08/46/9393ca20304119b40cbb5794479b77658b42e3db6d3bdd343e\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.2-py3-none-any.whl size=535570 sha256=574ac9e65f5544f478b23b55b792c52220c422b25823df59d70a928423cca727\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/08/13/2321517088bb2e95bfd0e45033bb9c923189e5b2078e0be4ef\n",
            "Successfully built gym torch-geometric\n",
            "Installing collected packages: isodate, yacs, rdflib, torch-sparse, torch-scatter, torch-geometric, torch, tensorboardX, split-folders, gym\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed gym-0.20.0 isodate-0.6.0 rdflib-6.0.2 split-folders-0.4.3 tensorboardX-2.4.1 torch-1.10.0+cu113 torch-geometric-2.0.2 torch-scatter-2.0.9 torch-sparse-0.6.12 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMMK1om7bzWs"
      },
      "source": [
        "datasets = {'uniform-random-3-sat': {'train': ['uf50-218', 'uuf50-218',\n",
        "                                               'uf100-430', 'uuf100-430'],\n",
        "                                     'val': ['uf50-218', 'uuf50-218',\n",
        "                                             'uf100-430', 'uuf100-430'],\n",
        "                                     'inner_test': ['uf50-218', 'uuf50-218',\n",
        "                                                    'uf100-430', 'uuf100-430'],\n",
        "                                     'test': ['uf250-1065', 'uuf250-1065']},\n",
        "            'graph-coloring': {'train': ['flat50-115'],\n",
        "                               'val': ['flat50-115'],\n",
        "                               'inner_test': ['flat50-115'],\n",
        "                               'test': ['flat30-60',\n",
        "                                        'flat75-180',\n",
        "                                        'flat100-239',\n",
        "                                        'flat125-301',\n",
        "                                        'flat150-360',\n",
        "                                        'flat175-417',\n",
        "                                        'flat200-479']}}"
      ],
      "id": "HMMK1om7bzWs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ypffdwwc60GL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39325f89-e01f-470c-b611-92f29a8a6c24"
      },
      "source": [
        "%cd GQSAT"
      ],
      "id": "Ypffdwwc60GL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/neuroSAT/GQSAT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hMESDiv-pWG"
      },
      "source": [
        "## Build C++"
      ],
      "id": "9hMESDiv-pWG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZF8lgna7tGm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ff7a314-d43c-4c11-e7eb-b7261436a343"
      },
      "source": [
        "%cd minisat\n",
        "!sudo ln -s --force /usr/local/lib/python3.7/dist-packages/numpy/core/include/numpy /usr/include/numpy  # https://stackoverflow.com/a/44935933/5555994\n",
        "!make distclean && CXXFLAGS=-w make && make python-wrap PYTHON=python3.7\n",
        "!apt install swig\n",
        "!swig -fastdispatch -c++ -python minisat/gym/GymSolver.i\n",
        "%cd .."
      ],
      "id": "nZF8lgna7tGm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/neuroSAT/GQSAT/minisat\n",
            "rm -f   build/release/minisat/core/Solver.o  build/release/minisat/core/Main.o  build/release/minisat/simp/SimpSolver.o  build/release/minisat/simp/Main.o  build/release/minisat/utils/System.o  build/release/minisat/utils/Options.o  build/release/minisat/gym/GymSolver.o   build/debug/minisat/core/Solver.o  build/debug/minisat/core/Main.o  build/debug/minisat/simp/SimpSolver.o  build/debug/minisat/simp/Main.o  build/debug/minisat/utils/System.o  build/debug/minisat/utils/Options.o  build/debug/minisat/gym/GymSolver.o   build/profile/minisat/core/Solver.o  build/profile/minisat/core/Main.o  build/profile/minisat/simp/SimpSolver.o  build/profile/minisat/simp/Main.o  build/profile/minisat/utils/System.o  build/profile/minisat/utils/Options.o  build/profile/minisat/gym/GymSolver.o   build/dynamic/minisat/core/Solver.o  build/dynamic/minisat/core/Main.o  build/dynamic/minisat/simp/SimpSolver.o  build/dynamic/minisat/simp/Main.o  build/dynamic/minisat/utils/System.o  build/dynamic/minisat/utils/Options.o  build/dynamic/minisat/gym/GymSolver.o \\\n",
            "            build/release/minisat/core/Solver.d  build/release/minisat/core/Main.d  build/release/minisat/simp/SimpSolver.d  build/release/minisat/simp/Main.d  build/release/minisat/utils/System.d  build/release/minisat/utils/Options.d  build/release/minisat/gym/GymSolver.d   build/debug/minisat/core/Solver.d  build/debug/minisat/core/Main.d  build/debug/minisat/simp/SimpSolver.d  build/debug/minisat/simp/Main.d  build/debug/minisat/utils/System.d  build/debug/minisat/utils/Options.d  build/debug/minisat/gym/GymSolver.d   build/profile/minisat/core/Solver.d  build/profile/minisat/core/Main.d  build/profile/minisat/simp/SimpSolver.d  build/profile/minisat/simp/Main.d  build/profile/minisat/utils/System.d  build/profile/minisat/utils/Options.d  build/profile/minisat/gym/GymSolver.d   build/dynamic/minisat/core/Solver.d  build/dynamic/minisat/core/Main.d  build/dynamic/minisat/simp/SimpSolver.d  build/dynamic/minisat/simp/Main.d  build/dynamic/minisat/utils/System.d  build/dynamic/minisat/utils/Options.d  build/dynamic/minisat/gym/GymSolver.d \\\n",
            "   build/release/bin/minisat_core build/release/bin/minisat  build/debug/bin/minisat_core build/debug/bin/minisat  build/profile/bin/minisat_core build/profile/bin/minisat  build/dynamic/bin/minisat_core build/dynamic/bin/minisat \\\n",
            "   build/release/lib/libminisat.a  build/debug/lib/libminisat.a  build/profile/lib/libminisat.a \\\n",
            "  build/dynamic/lib/libminisat.so.2.1.0\\\n",
            "  build/dynamic/lib/libminisat.so.2\\\n",
            "  build/dynamic/lib/libminisat.so\n",
            "rm -f config.mk\n",
            "Compiling: build/release/minisat/simp/Main.o\n",
            "Compiling: build/release/minisat/core/Solver.o\n",
            "Compiling: build/release/minisat/simp/SimpSolver.o\n",
            "Compiling: build/release/minisat/utils/System.o\n",
            "Compiling: build/release/minisat/utils/Options.o\n",
            "Compiling: build/release/minisat/gym/GymSolver.o\n",
            "Linking Static Library: build/release/lib/libminisat.a\n",
            "Linking Binary: build/release/bin/minisat\n",
            "Compiling: build/dynamic/minisat/core/Solver.o\n",
            "Compiling: build/dynamic/minisat/simp/SimpSolver.o\n",
            "Compiling: build/dynamic/minisat/utils/System.o\n",
            "Compiling: build/dynamic/minisat/utils/Options.o\n",
            "Compiling: build/dynamic/minisat/gym/GymSolver.o\n",
            "Linking Shared Library: build/dynamic/lib/libminisat.so.2.1.0\n",
            "g++ -O2 -fPIC -c minisat/gym/GymSolver_wrap.cxx -o minisat/gym/GymSolver_wrap.o -I. -I/usr/include/python3.7\n",
            "g++ -shared -o minisat/gym/_GymSolver.so build/dynamic/minisat/core/Solver.o build/dynamic/minisat/simp/SimpSolver.o build/dynamic/minisat/utils/System.o build/dynamic/minisat/utils/Options.o build/dynamic/minisat/gym/GymSolver.o minisat/gym/GymSolver_wrap.o /usr/lib/x86_64-linux-gnu/libz.so\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig3.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig3.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 1,100 kB of archives.\n",
            "After this operation, 5,822 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Fetched 1,100 kB in 1s (859 kB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 155222 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "/content/gdrive/My Drive/neuroSAT/GQSAT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH78QOrwiSVt"
      },
      "source": [
        "## Uniform Random 3-SAT\n",
        "\n",
        "We split *(u)uf50-218* and *(u)uf100-430* into three subsets: 800 training problems, 100 validation, and 100 test problems.\n",
        "\n",
        "For generalization experiments, we use 100 problems from all the other benchmarks.\n",
        "\n",
        "To evaluate the knowledge transfer properties of the trained models across different task families, we use 100 problems from all the *graph coloring* benchmarks."
      ],
      "id": "rH78QOrwiSVt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGI9Dj-FRXN4"
      },
      "source": [
        "PROBLEM_TYPE='uniform-random-3-sat'"
      ],
      "id": "HGI9Dj-FRXN4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WecpwjxDQGm"
      },
      "source": [
        "!bash train_val_test_split.sh \"$PROBLEM_TYPE\""
      ],
      "id": "7WecpwjxDQGm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqMVfDNR-_AV"
      },
      "source": [
        "### Add metadata for evaluation (train and validation set)"
      ],
      "id": "nqMVfDNR-_AV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bored-emphasis"
      },
      "source": [
        "for TRAIN_PROBLEM_NAME, VAL_PROBLEM_NAME in zip(datasets[PROBLEM_TYPE]['train'],\n",
        "                                                datasets[PROBLEM_TYPE]['val']):\n",
        "  !python add_metadata.py --eval-problems-paths ../data/\"$PROBLEM_TYPE\"/train/\"$TRAIN_PROBLEM_NAME\"\n",
        "  !python add_metadata.py --eval-problems-paths ../data/\"$PROBLEM_TYPE\"/val/\"$VAL_PROBLEM_NAME\""
      ],
      "id": "bored-emphasis",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTTzlCOm_W4d"
      },
      "source": [
        "### Train"
      ],
      "id": "MTTzlCOm_W4d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rising-death"
      },
      "source": [
        "for TRAIN_PROBLEM_NAME, VAL_PROBLEM_NAME in zip(datasets[PROBLEM_TYPE]['train'],\n",
        "                                                datasets[PROBLEM_TYPE]['val']):\n",
        "  !python dqn.py \\\n",
        "    --logdir log \\\n",
        "    --env-name sat-v0 \\\n",
        "    --train-problems-paths ../data/\"$PROBLEM_TYPE\"/train/\"$TRAIN_PROBLEM_NAME\" \\\n",
        "    --eval-problems-paths ../data/\"$PROBLEM_TYPE\"/val/\"$VAL_PROBLEM_NAME\" \\\n",
        "    --lr 0.00002 \\\n",
        "    --bsize 64 \\\n",
        "    --buffer-size 20000 \\\n",
        "    --eps-init 1.0 \\\n",
        "    --eps-final 0.01 \\\n",
        "    --gamma 0.99 \\\n",
        "    --eps-decay-steps 30000 \\\n",
        "    --batch-updates 50000 \\\n",
        "    --history-len 1 \\\n",
        "    --init-exploration-steps 5000 \\\n",
        "    --step-freq 4 \\\n",
        "    --target-update-freq 10 \\\n",
        "    --loss mse \\\n",
        "    --opt adam \\\n",
        "    --save-freq 500 \\\n",
        "    --grad_clip 1 \\\n",
        "    --grad_clip_norm_type 2 \\\n",
        "    --eval-freq 1000 \\\n",
        "    --eval-time-limit 3600 \\\n",
        "    --core-steps 4 \\\n",
        "    --expert-exploration-prob 0.0 \\\n",
        "    --priority_alpha 0.5 \\\n",
        "    --priority_beta 0.5 \\\n",
        "    --e2v-aggregator sum \\\n",
        "    --n_hidden 1 \\\n",
        "    --hidden_size 64 \\\n",
        "    --decoder_v_out_size 32 \\\n",
        "    --decoder_e_out_size 1 \\\n",
        "    --decoder_g_out_size 1 \\\n",
        "    --encoder_v_out_size 32 \\\n",
        "    --encoder_e_out_size 32 \\\n",
        "    --encoder_g_out_size 32 \\\n",
        "    --core_v_out_size 64 \\\n",
        "    --core_e_out_size 64 \\\n",
        "    --core_g_out_size 32 \\\n",
        "    --activation relu \\\n",
        "    --penalty_size 0.1 \\\n",
        "    --train_time_max_decisions_allowed 500 \\\n",
        "    --test_time_max_decisions_allowed 500 \\\n",
        "    --no_max_cap_fill_buffer \\\n",
        "    --lr_scheduler_gamma 1 \\\n",
        "    --lr_scheduler_frequency 3000 \\\n",
        "    --independent_block_layers 0 \\\n",
        "    --use_attention \\\n",
        "    --heads 3"
      ],
      "id": "rising-death",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIFGn6JJRgdO"
      },
      "source": [
        "### Add metadata for evaluation (test set)"
      ],
      "id": "UIFGn6JJRgdO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmGaDnimPYun"
      },
      "source": [
        "for PROBLEM_NAME in datasets[PROBLEM_TYPE]['inner_test']:\n",
        "  !python add_metadata.py --eval-problems-paths ../data/\"$PROBLEM_TYPE\"/test/\"$PROBLEM_NAME\"\n",
        "\n",
        "for PROBLEM_NAME in datasets[PROBLEM_TYPE]['test'] + datasets[PROBLEM_TYPE]['kt_test']:\n",
        "  !python add_metadata.py --eval-problems-paths ../data/\"$PROBLEM_TYPE\"/\"$PROBLEM_NAME\""
      ],
      "id": "BmGaDnimPYun",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H_5kfS8X3wm"
      },
      "source": [
        "### Evaluate"
      ],
      "id": "2H_5kfS8X3wm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptB7-ZibX3yE"
      },
      "source": [
        "models = {'uf50-218': ('Nov12_14-06-54_c42e8ad320d8', \n",
        "                       'model_50003'),\n",
        "          'uuf50-218': ('Nov12_20-35-32_c42e8ad320d8', \n",
        "                        'model_50006'),\n",
        "          'uf100-430': ('Nov13_03-55-51_c42e8ad320d8', \n",
        "                        'model_50085'),\n",
        "          'uuf100-430': ('Nov14_03-26-36_54337a27a809', \n",
        "                         'model_50003')}"
      ],
      "id": "ptB7-ZibX3yE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djZ1CBXbtzK8"
      },
      "source": [
        "We test these trained models on the inner test sets."
      ],
      "id": "djZ1CBXbtzK8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFxGGZdPX62D"
      },
      "source": [
        "for SAT_MODEL in models.keys():\n",
        "  for PROBLEM_NAME in datasets[PROBLEM_TYPE]['inner_test']:\n",
        "    # do not use models trained on unSAT problems to solve SAT ones\n",
        "    if not (SAT_MODEL.startswith('uuf') and PROBLEM_NAME.startswith('uf')):\n",
        "      for MODEL_DECISION in [10, 50, 100, 300, 500, 1000]:\n",
        "        MODEL_DIR = models[SAT_MODEL][0]\n",
        "        CHECKPOINT = models[SAT_MODEL][1]\n",
        "        !python evaluate.py \\\n",
        "          --logdir log \\\n",
        "          --env-name sat-v0 \\\n",
        "          --core-steps -1 \\\n",
        "          --eps-final 0.0 \\\n",
        "          --eval-time-limit 100000000000000 \\\n",
        "          --no_restarts \\\n",
        "          --test_time_max_decisions_allowed \"$MODEL_DECISION\" \\\n",
        "          --eval-problems-paths ../data/\"$PROBLEM_TYPE\"/test/\"$PROBLEM_NAME\" \\\n",
        "          --model-dir runs/\"$MODEL_DIR\" \\\n",
        "          --model-checkpoint \"$CHECKPOINT\".chkp \\\n",
        "          >> runs/\"$MODEL_DIR\"/\"$PROBLEM_NAME\"-gatqsat-max\"$MODEL_DECISION\".tsv"
      ],
      "id": "oFxGGZdPX62D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1Ux737RuaoN"
      },
      "source": [
        "We test the trained models on the outer test sets."
      ],
      "id": "C1Ux737RuaoN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huspm-fG4ruP"
      },
      "source": [
        "for SAT_MODEL in models.keys():\n",
        "  for PROBLEM_NAME in datasets[PROBLEM_TYPE]['test']:\n",
        "    # do not use models trained on unSAT problems to solve SAT ones\n",
        "    if not (SAT_MODEL.startswith('uuf') and PROBLEM_NAME.startswith('uf')):\n",
        "      for MODEL_DECISION in [10, 50, 100, 300, 500, 1000]:\n",
        "        MODEL_DIR = models[SAT_MODEL][0]\n",
        "        CHECKPOINT = models[SAT_MODEL][1]\n",
        "        !python evaluate.py \\\n",
        "          --logdir log \\\n",
        "          --env-name sat-v0 \\\n",
        "          --core-steps -1 \\\n",
        "          --eps-final 0.0 \\\n",
        "          --eval-time-limit 100000000000000 \\\n",
        "          --no_restarts \\\n",
        "          --test_time_max_decisions_allowed \"$MODEL_DECISION\" \\\n",
        "          --eval-problems-paths ../data/\"$PROBLEM_TYPE\"/\"$PROBLEM_NAME\" \\\n",
        "          --model-dir runs/\"$MODEL_DIR\" \\\n",
        "          --model-checkpoint \"$CHECKPOINT\".chkp \\\n",
        "          >> runs/\"$MODEL_DIR\"/\"$PROBLEM_NAME\"-gatqsat-max\"$MODEL_DECISION\".tsv"
      ],
      "id": "huspm-fG4ruP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H7bHgHblOit"
      },
      "source": [
        "We test the trained models on the *graph coloring* test sets, both inners and outers."
      ],
      "id": "-H7bHgHblOit"
    },
    {
      "cell_type": "code",
      "source": [
        "for SAT_MODEL in models.keys():\n",
        "  # do not use models trained on unSAT problems to solve SAT ones\n",
        "  if not SAT_MODEL.startswith('uuf'):\n",
        "    for PROBLEM_NAME in datasets['graph-coloring']['inner_test']:\n",
        "      for MODEL_DECISION in [10, 50, 100, 300, 500, 1000]:\n",
        "        MODEL_DIR = models[SAT_MODEL][0]\n",
        "        CHECKPOINT = models[SAT_MODEL][1]\n",
        "        !python evaluate.py \\\n",
        "          --logdir log \\\n",
        "          --env-name sat-v0 \\\n",
        "          --core-steps -1 \\\n",
        "          --eps-final 0.0 \\\n",
        "          --eval-time-limit 100000000000000 \\\n",
        "          --no_restarts \\\n",
        "          --test_time_max_decisions_allowed \"$MODEL_DECISION\" \\\n",
        "          --eval-problems-paths ../data/graph-coloring/test/\"$PROBLEM_NAME\" \\\n",
        "          --model-dir runs/\"$MODEL_DIR\" \\\n",
        "          --model-checkpoint \"$CHECKPOINT\".chkp \\\n",
        "          >> runs/\"$MODEL_DIR\"/\"$PROBLEM_NAME\"-gatqsat-max\"$MODEL_DECISION\".tsv"
      ],
      "metadata": {
        "id": "0WBwidOk9EIG"
      },
      "id": "0WBwidOk9EIG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfjCQpJzlNOS"
      },
      "source": [
        "for SAT_MODEL in models.keys():\n",
        "  # do not use models trained on unSAT problems to solve SAT ones\n",
        "  if not SAT_MODEL.startswith('uuf'):\n",
        "    for PROBLEM_NAME in datasets['graph-coloring']['test']:\n",
        "      for MODEL_DECISION in [10, 50, 100, 300, 500, 1000]:\n",
        "        MODEL_DIR = models[SAT_MODEL][0]\n",
        "        CHECKPOINT = models[SAT_MODEL][1]\n",
        "        !python evaluate.py \\\n",
        "          --logdir log \\\n",
        "          --env-name sat-v0 \\\n",
        "          --core-steps -1 \\\n",
        "          --eps-final 0.0 \\\n",
        "          --eval-time-limit 100000000000000 \\\n",
        "          --no_restarts \\\n",
        "          --test_time_max_decisions_allowed \"$MODEL_DECISION\" \\\n",
        "          --eval-problems-paths ../data/graph-coloring/\"$PROBLEM_NAME\" \\\n",
        "          --model-dir runs/\"$MODEL_DIR\" \\\n",
        "          --model-checkpoint \"$CHECKPOINT\".chkp \\\n",
        "          >> runs/\"$MODEL_DIR\"/\"$PROBLEM_NAME\"-gatqsat-max\"$MODEL_DECISION\".tsv"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "xfjCQpJzlNOS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYw40NNfjCDL"
      },
      "source": [
        "## Graph Coloring\n",
        "\n",
        "Graph coloring benchmarks have only 100 problems each, except for *flat50-115* which contains 1000, so we split it into three subsets: 800 training problems, 100 validation, and 100 test problems.\n",
        "\n",
        "For generalization experiments, we use 100 problems from all the other benchmarks."
      ],
      "id": "VYw40NNfjCDL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ods6xm8nRs1x"
      },
      "source": [
        "PROBLEM_TYPE='graph-coloring'"
      ],
      "id": "Ods6xm8nRs1x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crvN_WcWSFcb"
      },
      "source": [
        "!bash train_val_test_split.sh \"$PROBLEM_TYPE\""
      ],
      "execution_count": null,
      "outputs": [],
      "id": "crvN_WcWSFcb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-HPh7AVjCDM"
      },
      "source": [
        "### Add metadata for evaluation (train and validation set)"
      ],
      "id": "_-HPh7AVjCDM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n624I80gjCDM"
      },
      "source": [
        "for TRAIN_PROBLEM_NAME, VAL_PROBLEM_NAME in zip(datasets[PROBLEM_TYPE]['train'],\n",
        "                                                datasets[PROBLEM_TYPE]['val']):\n",
        "  !python add_metadata.py --eval-problems-paths ../data/\"$PROBLEM_TYPE\"/train/\"$TRAIN_PROBLEM_NAME\"\n",
        "  !python add_metadata.py --eval-problems-paths ../data/\"$PROBLEM_TYPE\"/val/\"$VAL_PROBLEM_NAME\""
      ],
      "id": "n624I80gjCDM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sloVrX1zjCDN"
      },
      "source": [
        "### Train"
      ],
      "id": "sloVrX1zjCDN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMwwOBfcjCDN"
      },
      "source": [
        "for TRAIN_PROBLEM_NAME, VAL_PROBLEM_NAME in zip(datasets[PROBLEM_TYPE]['train'],\n",
        "                                                datasets[PROBLEM_TYPE]['val']):\n",
        "  !python dqn.py \\\n",
        "    --logdir log \\\n",
        "    --env-name sat-v0 \\\n",
        "    --train-problems-paths ../data/\"$PROBLEM_TYPE\"/train/\"$TRAIN_PROBLEM_NAME\" \\\n",
        "    --eval-problems-paths ../data/\"$PROBLEM_TYPE\"/val/\"$VAL_PROBLEM_NAME\" \\\n",
        "    --lr 0.00002 \\\n",
        "    --bsize 64 \\\n",
        "    --buffer-size 20000 \\\n",
        "    --eps-init 1.0 \\\n",
        "    --eps-final 0.01 \\\n",
        "    --gamma 0.99 \\\n",
        "    --eps-decay-steps 30000 \\\n",
        "    --batch-updates 50000 \\\n",
        "    --history-len 1 \\\n",
        "    --init-exploration-steps 5000 \\\n",
        "    --step-freq 4 \\\n",
        "    --target-update-freq 10 \\\n",
        "    --loss mse \\\n",
        "    --opt adam \\\n",
        "    --save-freq 500 \\\n",
        "    --grad_clip 0.1 \\\n",
        "    --grad_clip_norm_type 2 \\\n",
        "    --eval-freq 1000 \\\n",
        "    --eval-time-limit 3600 \\\n",
        "    --core-steps 4 \\\n",
        "    --expert-exploration-prob 0.0 \\\n",
        "    --priority_alpha 0.5 \\\n",
        "    --priority_beta 0.5 \\\n",
        "    --e2v-aggregator sum \\\n",
        "    --n_hidden 1 \\\n",
        "    --hidden_size 64 \\\n",
        "    --decoder_v_out_size 32 \\\n",
        "    --decoder_e_out_size 1 \\\n",
        "    --decoder_g_out_size 1 \\\n",
        "    --encoder_v_out_size 32 \\\n",
        "    --encoder_e_out_size 32 \\\n",
        "    --encoder_g_out_size 32 \\\n",
        "    --core_v_out_size 64 \\\n",
        "    --core_e_out_size 64 \\\n",
        "    --core_g_out_size 32 \\\n",
        "    --activation relu \\\n",
        "    --penalty_size 0.1 \\\n",
        "    --train_time_max_decisions_allowed 500 \\\n",
        "    --test_time_max_decisions_allowed 500 \\\n",
        "    --no_max_cap_fill_buffer \\\n",
        "    --lr_scheduler_gamma 1 \\\n",
        "    --lr_scheduler_frequency 3000 \\\n",
        "    --independent_block_layers 0 \\\n",
        "    --use_attention \\\n",
        "    --heads 3"
      ],
      "id": "fMwwOBfcjCDN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MD6DHwwjCDN"
      },
      "source": [
        "### Add metadata for evaluation (test set)"
      ],
      "id": "1MD6DHwwjCDN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imKKT3snjCDO"
      },
      "source": [
        "for PROBLEM_NAME in datasets[PROBLEM_TYPE]['inner_test']:\n",
        "  !python add_metadata.py --eval-problems-paths ../data/\"$PROBLEM_TYPE\"/test/\"$PROBLEM_NAME\"\n",
        "\n",
        "for PROBLEM_NAME in datasets[PROBLEM_TYPE]['test']:\n",
        "  !python add_metadata.py --eval-problems-paths ../data/\"$PROBLEM_TYPE\"/\"$PROBLEM_NAME\""
      ],
      "id": "imKKT3snjCDO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am0iALxRjCDO"
      },
      "source": [
        "### Evaluate"
      ],
      "id": "am0iALxRjCDO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1GmjNk3jCDO"
      },
      "source": [
        "MODEL_DIR='Dec09_12-16-16_d4e65e7af705'\n",
        "CHECKPOINT='model_50001'"
      ],
      "id": "t1GmjNk3jCDO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0bYPPEdbAkZ"
      },
      "source": [
        "We test this trained model on the inner test set."
      ],
      "id": "v0bYPPEdbAkZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdSllBM8bAlc"
      },
      "source": [
        "for PROBLEM_NAME in datasets[PROBLEM_TYPE]['inner_test']:\n",
        "  for MODEL_DECISION in [10, 50, 100, 300, 500, 1000]:\n",
        "    !python evaluate.py \\\n",
        "      --logdir log \\\n",
        "      --env-name sat-v0 \\\n",
        "      --core-steps -1 \\\n",
        "      --eps-final 0.0 \\\n",
        "      --eval-time-limit 100000000000000 \\\n",
        "      --no_restarts \\\n",
        "      --test_time_max_decisions_allowed \"$MODEL_DECISION\" \\\n",
        "      --eval-problems-paths ../data/\"$PROBLEM_TYPE\"/test/\"$PROBLEM_NAME\" \\\n",
        "      --model-dir runs/\"$MODEL_DIR\" \\\n",
        "      --model-checkpoint \"$CHECKPOINT\".chkp \\\n",
        "      >> runs/\"$MODEL_DIR\"/\"$PROBLEM_NAME\"-gatqsat-max\"$MODEL_DECISION\".tsv"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "AdSllBM8bAlc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMW85WsMbAlf"
      },
      "source": [
        "We test the trained model on the outer test sets."
      ],
      "id": "zMW85WsMbAlf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YQTwDwvjCDO"
      },
      "source": [
        "for PROBLEM_NAME in datasets[PROBLEM_TYPE]['test']:\n",
        "  for MODEL_DECISION in [10, 50, 100, 300, 500, 1000]:\n",
        "    !python evaluate.py \\\n",
        "      --logdir log \\\n",
        "      --env-name sat-v0 \\\n",
        "      --core-steps -1 \\\n",
        "      --eps-final 0.0 \\\n",
        "      --eval-time-limit 100000000000000 \\\n",
        "      --no_restarts \\\n",
        "      --test_time_max_decisions_allowed \"$MODEL_DECISION\" \\\n",
        "      --eval-problems-paths ../data/\"$PROBLEM_TYPE\"/\"$PROBLEM_NAME\" \\\n",
        "      --model-dir runs/\"$MODEL_DIR\" \\\n",
        "      --model-checkpoint \"$CHECKPOINT\".chkp \\\n",
        "      >> runs/\"$MODEL_DIR\"/\"$PROBLEM_NAME\"-gatqsat-max\"$MODEL_DECISION\".tsv"
      ],
      "id": "7YQTwDwvjCDO",
      "execution_count": null,
      "outputs": []
    }
  ]
}