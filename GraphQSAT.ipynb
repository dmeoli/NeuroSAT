{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "name": "GraphQSAT.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmeoli/NeuroSAT/blob/master/GraphQSAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "md4JZ9dNr-cj",
        "outputId": "b7391a92-bb5b-461a-d75c-5c038313ac77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "id": "md4JZ9dNr-cj",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSSjiEf1sjTf",
        "outputId": "69c38d7f-4b32-442a-bdab-389af87e0123",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd gdrive/My Drive"
      ],
      "id": "PSSjiEf1sjTf",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQDmH68WssAm",
        "outputId": "c1b019fc-f0a2-4540-8232-3b8f33d9f5b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir('neuroSAT'):\n",
        "  !git clone --recurse-submodules https://github.com/dmeoli/neuroSAT\n",
        "  %cd neuroSAT\n",
        "else:\n",
        "  %cd neuroSAT\n",
        "  !git pull"
      ],
      "id": "wQDmH68WssAm",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/neuroSAT\n",
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDLLNcrEt_il",
        "outputId": "60464123-b4f4-4832-ea8d-f257e52c9e5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "id": "FDLLNcrEt_il",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/cu113/torch_stable.html, https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
            "Collecting gym==0.20.0\n",
            "  Downloading gym-0.20.0.tar.gz (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.19.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (3.13)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.4.1)\n",
            "Collecting split-folders\n",
            "  Downloading split_folders-0.4.3-py3-none-any.whl (7.4 kB)\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.4.1-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 66.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (2.7.0)\n",
            "Collecting torch==1.10.0+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torch-1.10.0%2Bcu113-cp37-cp37m-linux_x86_64.whl (1821.5 MB)\n",
            "\u001b[K     |██████████████▋                 | 834.1 MB 133.0 MB/s eta 0:00:08tcmalloc: large alloc 1147494400 bytes == 0x564cc3796000 @  0x7fee6e851615 0x564cbfe0c4cc 0x564cbfeec47a 0x564cbfe0f2ed 0x564cbff00e1d 0x564cbfe82e99 0x564cbfe7d9ee 0x564cbfe10bda 0x564cbfe82d00 0x564cbfe7d9ee 0x564cbfe10bda 0x564cbfe7f737 0x564cbff01c66 0x564cbfe7edaf 0x564cbff01c66 0x564cbfe7edaf 0x564cbff01c66 0x564cbfe7edaf 0x564cbfe11039 0x564cbfe54409 0x564cbfe0fc52 0x564cbfe82c25 0x564cbfe7d9ee 0x564cbfe10bda 0x564cbfe7f737 0x564cbfe7d9ee 0x564cbfe10bda 0x564cbfe7e915 0x564cbfe10afa 0x564cbfe7ec0d 0x564cbfe7d9ee\n",
            "\u001b[K     |██████████████████▌             | 1055.7 MB 1.2 MB/s eta 0:10:46tcmalloc: large alloc 1434370048 bytes == 0x564d07dec000 @  0x7fee6e851615 0x564cbfe0c4cc 0x564cbfeec47a 0x564cbfe0f2ed 0x564cbff00e1d 0x564cbfe82e99 0x564cbfe7d9ee 0x564cbfe10bda 0x564cbfe82d00 0x564cbfe7d9ee 0x564cbfe10bda 0x564cbfe7f737 0x564cbff01c66 0x564cbfe7edaf 0x564cbff01c66 0x564cbfe7edaf 0x564cbff01c66 0x564cbfe7edaf 0x564cbfe11039 0x564cbfe54409 0x564cbfe0fc52 0x564cbfe82c25 0x564cbfe7d9ee 0x564cbfe10bda 0x564cbfe7f737 0x564cbfe7d9ee 0x564cbfe10bda 0x564cbfe7e915 0x564cbfe10afa 0x564cbfe7ec0d 0x564cbfe7d9ee\n",
            "\u001b[K     |███████████████████████▌        | 1336.2 MB 1.2 MB/s eta 0:06:56tcmalloc: large alloc 1792966656 bytes == 0x564d5d5d8000 @  0x7fee6e851615 0x564cbfe0c4cc 0x564cbfeec47a 0x564cbfe0f2ed 0x564cbff00e1d 0x564cbfe82e99 0x564cbfe7d9ee 0x564cbfe10bda 0x564cbfe82d00 0x564cbfe7d9ee 0x564cbfe10bda 0x564cbfe7f737 0x564cbff01c66 0x564cbfe7edaf 0x564cbff01c66 0x564cbfe7edaf 0x564cbff01c66 0x564cbfe7edaf 0x564cbfe11039 0x564cbfe54409 0x564cbfe0fc52 0x564cbfe82c25 0x564cbfe7d9ee 0x564cbfe10bda 0x564cbfe7f737 0x564cbfe7d9ee 0x564cbfe10bda 0x564cbfe7e915 0x564cbfe10afa 0x564cbfe7ec0d 0x564cbfe7d9ee\n",
            "\u001b[K     |█████████████████████████████▊  | 1691.1 MB 1.1 MB/s eta 0:01:56tcmalloc: large alloc 2241208320 bytes == 0x564cc3796000 @  0x7fee6e851615 0x564cbfe0c4cc 0x564cbfeec47a 0x564cbfe0f2ed 0x564cbff00e1d 0x564cbfe82e99 0x564cbfe7d9ee 0x564cbfe10bda 0x564cbfe82d00 0x564cbfe7d9ee 0x564cbfe10bda 0x564cbfe7f737 0x564cbff01c66 0x564cbfe7edaf 0x564cbff01c66 0x564cbfe7edaf 0x564cbff01c66 0x564cbfe7edaf 0x564cbfe11039 0x564cbfe54409 0x564cbfe0fc52 0x564cbfe82c25 0x564cbfe7d9ee 0x564cbfe10bda 0x564cbfe7f737 0x564cbfe7d9ee 0x564cbfe10bda 0x564cbfe7e915 0x564cbfe10afa 0x564cbfe7ec0d 0x564cbfe7d9ee\n",
            "\u001b[K     |████████████████████████████████| 1821.5 MB 1.2 MB/s eta 0:00:01tcmalloc: large alloc 1821458432 bytes == 0x564d490f8000 @  0x7fee6e8501e7 0x564cbfe42067 0x564cbfe0c4cc 0x564cbfeec47a 0x564cbfe0f2ed 0x564cbff00e1d 0x564cbfe82e99 0x564cbfe7d9ee 0x564cbfe10bda 0x564cbfe7ec0d 0x564cbfe7d9ee 0x564cbfe10bda 0x564cbfe7ec0d 0x564cbfe7d9ee 0x564cbfe10bda 0x564cbfe7ec0d 0x564cbfe7d9ee 0x564cbfe10bda 0x564cbfe7ec0d 0x564cbfe7d9ee 0x564cbfe10bda 0x564cbfe7ec0d 0x564cbfe10afa 0x564cbfe7ec0d 0x564cbfe7d9ee 0x564cbfe10bda 0x564cbfe7f737 0x564cbfe7d9ee 0x564cbfe10bda 0x564cbfe7f737 0x564cbfe7d9ee\n",
            "tcmalloc: large alloc 2276827136 bytes == 0x564db5a0c000 @  0x7fee6e851615 0x564cbfe0c4cc 0x564cbfeec47a 0x564cbfe0f2ed 0x564cbff00e1d 0x564cbfe82e99 0x564cbfe7d9ee 0x564cbfe10bda 0x564cbfe7ec0d 0x564cbfe7d9ee 0x564cbfe10bda 0x564cbfe7ec0d 0x564cbfe7d9ee 0x564cbfe10bda 0x564cbfe7ec0d 0x564cbfe7d9ee 0x564cbfe10bda 0x564cbfe7ec0d 0x564cbfe7d9ee 0x564cbfe10bda 0x564cbfe7ec0d 0x564cbfe10afa 0x564cbfe7ec0d 0x564cbfe7d9ee 0x564cbfe10bda 0x564cbfe7f737 0x564cbfe7d9ee 0x564cbfe10bda 0x564cbfe7f737 0x564cbfe7d9ee 0x564cbfe11271\n",
            "\u001b[K     |████████████████████████████████| 1821.5 MB 5.7 kB/s \n",
            "\u001b[?25hCollecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 2.7 MB/s \n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_sparse-0.6.12-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 58.9 MB/s \n",
            "\u001b[?25hCollecting torch-geometric\n",
            "  Downloading torch_geometric-2.0.3.tar.gz (370 kB)\n",
            "\u001b[K     |████████████████████████████████| 370 kB 74.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.20.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0+cu113->-r requirements.txt (line 9)) (3.10.0.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX->-r requirements.txt (line 6)) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX->-r requirements.txt (line 6)) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (0.22.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (0.37.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (2.7.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (3.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (1.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (1.13.3)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (12.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (1.42.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (0.12.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (2.7.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (2.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 7)) (0.4.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->-r requirements.txt (line 7)) (1.5.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow->-r requirements.txt (line 7)) (3.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric->-r requirements.txt (line 13)) (4.62.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric->-r requirements.txt (line 13)) (2.6.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric->-r requirements.txt (line 13)) (1.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric->-r requirements.txt (line 13)) (1.1.5)\n",
            "Collecting rdflib\n",
            "  Downloading rdflib-6.1.1-py3-none-any.whl (482 kB)\n",
            "\u001b[K     |████████████████████████████████| 482 kB 68.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric->-r requirements.txt (line 13)) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric->-r requirements.txt (line 13)) (2.11.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric->-r requirements.txt (line 13)) (3.0.6)\n",
            "Collecting yacs\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric->-r requirements.txt (line 13)) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric->-r requirements.txt (line 13)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric->-r requirements.txt (line 13)) (2.8.2)\n",
            "Collecting isodate\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 783 kB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric->-r requirements.txt (line 13)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric->-r requirements.txt (line 13)) (3.0.0)\n",
            "Building wheels for collected packages: gym, torch-geometric\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.20.0-py3-none-any.whl size=1650479 sha256=5213ac4aba93f2e455a102194da33649ae2103b73be13f9e22135b155e56b536\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/08/46/9393ca20304119b40cbb5794479b77658b42e3db6d3bdd343e\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.3-py3-none-any.whl size=581969 sha256=8e6379070913ee81a971c7fb5529b460166f8aaab7f7a54e68cacfb7a658063e\n",
            "  Stored in directory: /root/.cache/pip/wheels/c3/2a/58/87ce0508964d4def1aafb92750c4f3ac77038efd1b9a89dcf5\n",
            "Successfully built gym torch-geometric\n",
            "Installing collected packages: isodate, yacs, rdflib, torch-sparse, torch-scatter, torch-geometric, torch, tensorboardX, split-folders, gym\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed gym-0.20.0 isodate-0.6.1 rdflib-6.1.1 split-folders-0.4.3 tensorboardX-2.4.1 torch-1.10.0+cu113 torch-geometric-2.0.3 torch-scatter-2.0.9 torch-sparse-0.6.12 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMMK1om7bzWs"
      },
      "source": [
        "datasets = {'uniform-random-3-sat': {'train': ['uf50-218', 'uuf50-218',\n",
        "                                               'uf100-430', 'uuf100-430'],\n",
        "                                     'val': ['uf50-218', 'uuf50-218',\n",
        "                                             'uf100-430', 'uuf100-430'],\n",
        "                                     'inner_test': ['uf50-218', 'uuf50-218',\n",
        "                                                    'uf100-430', 'uuf100-430'],\n",
        "                                     'test': ['uf250-1065', 'uuf250-1065']},\n",
        "            'graph-coloring': {'train': ['flat50-115'],\n",
        "                               'val': ['flat50-115'],\n",
        "                               'inner_test': ['flat50-115'],\n",
        "                               'test': ['flat30-60',\n",
        "                                        'flat75-180',\n",
        "                                        'flat100-239',\n",
        "                                        'flat125-301',\n",
        "                                        'flat150-360',\n",
        "                                        'flat175-417',\n",
        "                                        'flat200-479']}}"
      ],
      "id": "HMMK1om7bzWs",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ypffdwwc60GL",
        "outputId": "e7392f9b-d22b-40bf-a7b4-abcc3c483eb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd GQSAT"
      ],
      "id": "Ypffdwwc60GL",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/neuroSAT/GQSAT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hMESDiv-pWG"
      },
      "source": [
        "## Build C++"
      ],
      "id": "9hMESDiv-pWG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZF8lgna7tGm",
        "outputId": "114b0610-1f7d-4b3e-97e5-ee3c39c010a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd minisat\n",
        "!sudo ln -s --force /usr/local/lib/python3.7/dist-packages/numpy/core/include/numpy /usr/include/numpy  # https://stackoverflow.com/a/44935933/5555994\n",
        "!make distclean && CXXFLAGS=-w make && make python-wrap PYTHON=python3.7\n",
        "!apt install swig\n",
        "!swig -fastdispatch -c++ -python minisat/gym/GymSolver.i\n",
        "%cd .."
      ],
      "id": "nZF8lgna7tGm",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/neuroSAT/GQSAT/minisat\n",
            "rm -f   build/release/minisat/core/Solver.o  build/release/minisat/core/Main.o  build/release/minisat/simp/SimpSolver.o  build/release/minisat/simp/Main.o  build/release/minisat/utils/System.o  build/release/minisat/utils/Options.o  build/release/minisat/gym/GymSolver.o   build/debug/minisat/core/Solver.o  build/debug/minisat/core/Main.o  build/debug/minisat/simp/SimpSolver.o  build/debug/minisat/simp/Main.o  build/debug/minisat/utils/System.o  build/debug/minisat/utils/Options.o  build/debug/minisat/gym/GymSolver.o   build/profile/minisat/core/Solver.o  build/profile/minisat/core/Main.o  build/profile/minisat/simp/SimpSolver.o  build/profile/minisat/simp/Main.o  build/profile/minisat/utils/System.o  build/profile/minisat/utils/Options.o  build/profile/minisat/gym/GymSolver.o   build/dynamic/minisat/core/Solver.o  build/dynamic/minisat/core/Main.o  build/dynamic/minisat/simp/SimpSolver.o  build/dynamic/minisat/simp/Main.o  build/dynamic/minisat/utils/System.o  build/dynamic/minisat/utils/Options.o  build/dynamic/minisat/gym/GymSolver.o \\\n",
            "            build/release/minisat/core/Solver.d  build/release/minisat/core/Main.d  build/release/minisat/simp/SimpSolver.d  build/release/minisat/simp/Main.d  build/release/minisat/utils/System.d  build/release/minisat/utils/Options.d  build/release/minisat/gym/GymSolver.d   build/debug/minisat/core/Solver.d  build/debug/minisat/core/Main.d  build/debug/minisat/simp/SimpSolver.d  build/debug/minisat/simp/Main.d  build/debug/minisat/utils/System.d  build/debug/minisat/utils/Options.d  build/debug/minisat/gym/GymSolver.d   build/profile/minisat/core/Solver.d  build/profile/minisat/core/Main.d  build/profile/minisat/simp/SimpSolver.d  build/profile/minisat/simp/Main.d  build/profile/minisat/utils/System.d  build/profile/minisat/utils/Options.d  build/profile/minisat/gym/GymSolver.d   build/dynamic/minisat/core/Solver.d  build/dynamic/minisat/core/Main.d  build/dynamic/minisat/simp/SimpSolver.d  build/dynamic/minisat/simp/Main.d  build/dynamic/minisat/utils/System.d  build/dynamic/minisat/utils/Options.d  build/dynamic/minisat/gym/GymSolver.d \\\n",
            "   build/release/bin/minisat_core build/release/bin/minisat  build/debug/bin/minisat_core build/debug/bin/minisat  build/profile/bin/minisat_core build/profile/bin/minisat  build/dynamic/bin/minisat_core build/dynamic/bin/minisat \\\n",
            "   build/release/lib/libminisat.a  build/debug/lib/libminisat.a  build/profile/lib/libminisat.a \\\n",
            "  build/dynamic/lib/libminisat.so.2.1.0\\\n",
            "  build/dynamic/lib/libminisat.so.2\\\n",
            "  build/dynamic/lib/libminisat.so\n",
            "rm -f config.mk\n",
            "Compiling: build/release/minisat/simp/Main.o\n",
            "Compiling: build/release/minisat/core/Solver.o\n",
            "Compiling: build/release/minisat/simp/SimpSolver.o\n",
            "Compiling: build/release/minisat/utils/System.o\n",
            "Compiling: build/release/minisat/utils/Options.o\n",
            "Compiling: build/release/minisat/gym/GymSolver.o\n",
            "Linking Static Library: build/release/lib/libminisat.a\n",
            "Linking Binary: build/release/bin/minisat\n",
            "Compiling: build/dynamic/minisat/core/Solver.o\n",
            "Compiling: build/dynamic/minisat/simp/SimpSolver.o\n",
            "Compiling: build/dynamic/minisat/utils/System.o\n",
            "Compiling: build/dynamic/minisat/utils/Options.o\n",
            "Compiling: build/dynamic/minisat/gym/GymSolver.o\n",
            "Linking Shared Library: build/dynamic/lib/libminisat.so.2.1.0\n",
            "g++ -O2 -fPIC -c minisat/gym/GymSolver_wrap.cxx -o minisat/gym/GymSolver_wrap.o -I. -I/usr/include/python3.7\n",
            "g++ -shared -o minisat/gym/_GymSolver.so build/dynamic/minisat/core/Solver.o build/dynamic/minisat/simp/SimpSolver.o build/dynamic/minisat/utils/System.o build/dynamic/minisat/utils/Options.o build/dynamic/minisat/gym/GymSolver.o minisat/gym/GymSolver_wrap.o /usr/lib/x86_64-linux-gnu/libz.so\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig3.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig3.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 1,100 kB of archives.\n",
            "After this operation, 5,822 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Fetched 1,100 kB in 2s (489 kB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 155222 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "/content/gdrive/My Drive/neuroSAT/GQSAT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH78QOrwiSVt"
      },
      "source": [
        "## Uniform Random 3-SAT\n",
        "\n",
        "We split *(u)uf50-218* and *(u)uf100-430* into three subsets: 800 training problems, 100 validation, and 100 test problems.\n",
        "\n",
        "For generalization experiments, we use 100 problems from all the other benchmarks.\n",
        "\n",
        "To evaluate the knowledge transfer properties of the trained models across different task families, we use 100 problems from all the *graph coloring* benchmarks."
      ],
      "id": "rH78QOrwiSVt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGI9Dj-FRXN4"
      },
      "source": [
        "PROBLEM_TYPE='uniform-random-3-sat'"
      ],
      "id": "HGI9Dj-FRXN4",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WecpwjxDQGm"
      },
      "source": [
        "!bash train_val_test_split.sh \"$PROBLEM_TYPE\""
      ],
      "id": "7WecpwjxDQGm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqMVfDNR-_AV"
      },
      "source": [
        "### Add metadata for evaluation (train and validation set)"
      ],
      "id": "nqMVfDNR-_AV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bored-emphasis"
      },
      "source": [
        "for TRAIN_PROBLEM_NAME, VAL_PROBLEM_NAME in zip(datasets[PROBLEM_TYPE]['train'],\n",
        "                                                datasets[PROBLEM_TYPE]['val']):\n",
        "  !python add_metadata.py --eval-problems-paths ../data/\"$PROBLEM_TYPE\"/train/\"$TRAIN_PROBLEM_NAME\"\n",
        "  !python add_metadata.py --eval-problems-paths ../data/\"$PROBLEM_TYPE\"/val/\"$VAL_PROBLEM_NAME\""
      ],
      "id": "bored-emphasis",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTTzlCOm_W4d"
      },
      "source": [
        "### Train"
      ],
      "id": "MTTzlCOm_W4d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rising-death"
      },
      "source": [
        "for TRAIN_PROBLEM_NAME, VAL_PROBLEM_NAME in zip(datasets[PROBLEM_TYPE]['train'],\n",
        "                                                datasets[PROBLEM_TYPE]['val']):\n",
        "  !python dqn.py \\\n",
        "    --logdir log \\\n",
        "    --env-name sat-v0 \\\n",
        "    --train-problems-paths ../data/\"$PROBLEM_TYPE\"/train/\"$TRAIN_PROBLEM_NAME\" \\\n",
        "    --eval-problems-paths ../data/\"$PROBLEM_TYPE\"/val/\"$VAL_PROBLEM_NAME\" \\\n",
        "    --lr 0.00002 \\\n",
        "    --bsize 64 \\\n",
        "    --buffer-size 20000 \\\n",
        "    --eps-init 1.0 \\\n",
        "    --eps-final 0.01 \\\n",
        "    --gamma 0.99 \\\n",
        "    --eps-decay-steps 30000 \\\n",
        "    --batch-updates 50000 \\\n",
        "    --history-len 1 \\\n",
        "    --init-exploration-steps 5000 \\\n",
        "    --step-freq 4 \\\n",
        "    --target-update-freq 10 \\\n",
        "    --loss mse \\\n",
        "    --opt adam \\\n",
        "    --save-freq 500 \\\n",
        "    --grad_clip 1 \\\n",
        "    --grad_clip_norm_type 2 \\\n",
        "    --eval-freq 1000 \\\n",
        "    --eval-time-limit 3600 \\\n",
        "    --core-steps 4 \\\n",
        "    --expert-exploration-prob 0.0 \\\n",
        "    --priority_alpha 0.5 \\\n",
        "    --priority_beta 0.5 \\\n",
        "    --e2v-aggregator sum \\\n",
        "    --n_hidden 1 \\\n",
        "    --hidden_size 64 \\\n",
        "    --decoder_v_out_size 32 \\\n",
        "    --decoder_e_out_size 1 \\\n",
        "    --decoder_g_out_size 1 \\\n",
        "    --encoder_v_out_size 32 \\\n",
        "    --encoder_e_out_size 32 \\\n",
        "    --encoder_g_out_size 32 \\\n",
        "    --core_v_out_size 64 \\\n",
        "    --core_e_out_size 64 \\\n",
        "    --core_g_out_size 32 \\\n",
        "    --activation relu \\\n",
        "    --penalty_size 0.1 \\\n",
        "    --train_time_max_decisions_allowed 500 \\\n",
        "    --test_time_max_decisions_allowed 500 \\\n",
        "    --no_max_cap_fill_buffer \\\n",
        "    --lr_scheduler_gamma 1 \\\n",
        "    --lr_scheduler_frequency 3000 \\\n",
        "    --independent_block_layers 0"
      ],
      "id": "rising-death",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIFGn6JJRgdO"
      },
      "source": [
        "### Add metadata for evaluation (test set)"
      ],
      "id": "UIFGn6JJRgdO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmGaDnimPYun"
      },
      "source": [
        "for PROBLEM_NAME in datasets[PROBLEM_TYPE]['inner_test']:\n",
        "  !python add_metadata.py --eval-problems-paths ../data/\"$PROBLEM_TYPE\"/test/\"$PROBLEM_NAME\"\n",
        "\n",
        "for PROBLEM_NAME in datasets['graph-coloring']['inner_test']:\n",
        "  !python add_metadata.py --eval-problems-paths ../data/\"$PROBLEM_TYPE\"/test/\"$PROBLEM_NAME\"\n",
        "\n",
        "for PROBLEM_NAME in datasets['graph-coloring']['test']:\n",
        "  !python add_metadata.py --eval-problems-paths ../data/\"$PROBLEM_TYPE\"/\"$PROBLEM_NAME\""
      ],
      "id": "BmGaDnimPYun",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H_5kfS8X3wm"
      },
      "source": [
        "### Evaluate"
      ],
      "id": "2H_5kfS8X3wm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptB7-ZibX3yE"
      },
      "source": [
        "models = {'uf50-218': ('Dec21_01-59-59_6bed2aa9b612',\n",
        "                       'model_50002'),\n",
        "          'uuf50-218': ('Dec21_14-55-50_5eccdc34d583',\n",
        "                        'model_50007'),\n",
        "          'uf100-430': ('Dec23_01-48-54_90582559eea7',\n",
        "                        'model_50028'),\n",
        "          'uuf100-430': ('Dec23_14-42-44_90582559eea7',\n",
        "                         'model_50037')}"
      ],
      "id": "ptB7-ZibX3yE",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djZ1CBXbtzK8"
      },
      "source": [
        "We test these trained models on the inner test sets."
      ],
      "id": "djZ1CBXbtzK8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFxGGZdPX62D"
      },
      "source": [
        "for SAT_MODEL in models.keys():\n",
        "  for PROBLEM_NAME in datasets[PROBLEM_TYPE]['inner_test']:\n",
        "    # do not use models trained on unSAT problems to solve SAT ones\n",
        "    if not (SAT_MODEL.startswith('uuf') and PROBLEM_NAME.startswith('uf')):\n",
        "      for MODEL_DECISION in [10, 50, 100, 300, 500, 1000]:\n",
        "        MODEL_DIR = models[SAT_MODEL][0]\n",
        "        CHECKPOINT = models[SAT_MODEL][1]\n",
        "        !python evaluate.py \\\n",
        "          --logdir log \\\n",
        "          --env-name sat-v0 \\\n",
        "          --core-steps -1 \\\n",
        "          --eps-final 0.0 \\\n",
        "          --eval-time-limit 100000000000000 \\\n",
        "          --no_restarts \\\n",
        "          --test_time_max_decisions_allowed \"$MODEL_DECISION\" \\\n",
        "          --eval-problems-paths ../data/\"$PROBLEM_TYPE\"/test/\"$PROBLEM_NAME\" \\\n",
        "          --model-dir runs/\"$MODEL_DIR\" \\\n",
        "          --model-checkpoint \"$CHECKPOINT\".chkp \\\n",
        "          >> runs/\"$MODEL_DIR\"/\"$PROBLEM_NAME\"-graphqsat-max\"$MODEL_DECISION\".tsv"
      ],
      "id": "oFxGGZdPX62D",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1Ux737RuaoN"
      },
      "source": [
        "We test the trained models on the outer test sets."
      ],
      "id": "C1Ux737RuaoN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huspm-fG4ruP"
      },
      "source": [
        "for SAT_MODEL in models.keys():\n",
        "  for PROBLEM_NAME in datasets[PROBLEM_TYPE]['test']:\n",
        "    # do not use models trained on unSAT problems to solve SAT ones\n",
        "    if not (SAT_MODEL.startswith('uuf') and PROBLEM_NAME.startswith('uf')):\n",
        "      for MODEL_DECISION in [10, 50, 100, 300, 500, 1000]:\n",
        "        MODEL_DIR = models[SAT_MODEL][0]\n",
        "        CHECKPOINT = models[SAT_MODEL][1]\n",
        "        !python evaluate.py \\\n",
        "          --logdir log \\\n",
        "          --env-name sat-v0 \\\n",
        "          --core-steps -1 \\\n",
        "          --eps-final 0.0 \\\n",
        "          --eval-time-limit 100000000000000 \\\n",
        "          --no_restarts \\\n",
        "          --test_time_max_decisions_allowed \"$MODEL_DECISION\" \\\n",
        "          --eval-problems-paths ../data/\"$PROBLEM_TYPE\"/\"$PROBLEM_NAME\" \\\n",
        "          --model-dir runs/\"$MODEL_DIR\" \\\n",
        "          --model-checkpoint \"$CHECKPOINT\".chkp \\\n",
        "          >> runs/\"$MODEL_DIR\"/\"$PROBLEM_NAME\"-graphqsat-max\"$MODEL_DECISION\".tsv"
      ],
      "id": "huspm-fG4ruP",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H7bHgHblOit"
      },
      "source": [
        "We test the trained models on the *graph coloring* test sets, both inners and outers."
      ],
      "id": "-H7bHgHblOit"
    },
    {
      "cell_type": "code",
      "source": [
        "for SAT_MODEL in models.keys():\n",
        "  # do not use models trained on unSAT problems to solve SAT ones\n",
        "  if not SAT_MODEL.startswith('uuf'):\n",
        "    for PROBLEM_NAME in datasets['graph-coloring']['inner_test']:\n",
        "      for MODEL_DECISION in [10, 50, 100, 300, 500, 1000]:\n",
        "        MODEL_DIR = models[SAT_MODEL][0]\n",
        "        CHECKPOINT = models[SAT_MODEL][1]\n",
        "        !python evaluate.py \\\n",
        "          --logdir log \\\n",
        "          --env-name sat-v0 \\\n",
        "          --core-steps -1 \\\n",
        "          --eps-final 0.0 \\\n",
        "          --eval-time-limit 100000000000000 \\\n",
        "          --no_restarts \\\n",
        "          --test_time_max_decisions_allowed \"$MODEL_DECISION\" \\\n",
        "          --eval-problems-paths ../data/graph-coloring/test/\"$PROBLEM_NAME\" \\\n",
        "          --model-dir runs/\"$MODEL_DIR\" \\\n",
        "          --model-checkpoint \"$CHECKPOINT\".chkp \\\n",
        "          >> runs/\"$MODEL_DIR\"/\"$PROBLEM_NAME\"-graphqsat-max\"$MODEL_DECISION\".tsv"
      ],
      "metadata": {
        "id": "uO23dEOf9hNr"
      },
      "id": "uO23dEOf9hNr",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfjCQpJzlNOS"
      },
      "source": [
        "for SAT_MODEL in models.keys():\n",
        "  # do not use models trained on unSAT problems to solve SAT ones\n",
        "  if not SAT_MODEL.startswith('uuf'):\n",
        "    for PROBLEM_NAME in datasets['graph-coloring']['test']:\n",
        "      for MODEL_DECISION in [10, 50, 100, 300, 500, 1000]:\n",
        "        MODEL_DIR = models[SAT_MODEL][0]\n",
        "        CHECKPOINT = models[SAT_MODEL][1]\n",
        "        !python evaluate.py \\\n",
        "          --logdir log \\\n",
        "          --env-name sat-v0 \\\n",
        "          --core-steps -1 \\\n",
        "          --eps-final 0.0 \\\n",
        "          --eval-time-limit 100000000000000 \\\n",
        "          --no_restarts \\\n",
        "          --test_time_max_decisions_allowed \"$MODEL_DECISION\" \\\n",
        "          --eval-problems-paths ../data/graph-coloring/\"$PROBLEM_NAME\" \\\n",
        "          --model-dir runs/\"$MODEL_DIR\" \\\n",
        "          --model-checkpoint \"$CHECKPOINT\".chkp \\\n",
        "          >> runs/\"$MODEL_DIR\"/\"$PROBLEM_NAME\"-graphqsat-max\"$MODEL_DECISION\".tsv"
      ],
      "id": "xfjCQpJzlNOS",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYw40NNfjCDL"
      },
      "source": [
        "## Graph Coloring\n",
        "\n",
        "Graph coloring benchmarks have only 100 problems each, except for *flat50-115* which contains 1000, so we split it into three subsets: 800 training problems, 100 validation, and 100 test problems.\n",
        "\n",
        "For generalization experiments, we use 100 problems from all the other benchmarks."
      ],
      "id": "VYw40NNfjCDL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ods6xm8nRs1x"
      },
      "source": [
        "PROBLEM_TYPE='graph-coloring'"
      ],
      "id": "Ods6xm8nRs1x",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG9OBAFGSCU8"
      },
      "source": [
        "!bash train_val_test_split.sh \"$PROBLEM_TYPE\""
      ],
      "id": "QG9OBAFGSCU8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-HPh7AVjCDM"
      },
      "source": [
        "### Add metadata for evaluation (train and validation set)"
      ],
      "id": "_-HPh7AVjCDM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n624I80gjCDM"
      },
      "source": [
        "for TRAIN_PROBLEM_NAME, VAL_PROBLEM_NAME in zip(datasets[PROBLEM_TYPE]['train'],\n",
        "                                                datasets[PROBLEM_TYPE]['val']):\n",
        "  !python add_metadata.py --eval-problems-paths ../data/\"$PROBLEM_TYPE\"/train/\"$TRAIN_PROBLEM_NAME\"\n",
        "  !python add_metadata.py --eval-problems-paths ../data/\"$PROBLEM_TYPE\"/val/\"$VAL_PROBLEM_NAME\""
      ],
      "id": "n624I80gjCDM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sloVrX1zjCDN"
      },
      "source": [
        "### Train"
      ],
      "id": "sloVrX1zjCDN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMwwOBfcjCDN"
      },
      "source": [
        "for TRAIN_PROBLEM_NAME, VAL_PROBLEM_NAME in zip(datasets[PROBLEM_TYPE]['train'],\n",
        "                                                datasets[PROBLEM_TYPE]['val']):\n",
        "  !python dqn.py \\\n",
        "    --logdir log \\\n",
        "    --env-name sat-v0 \\\n",
        "    --train-problems-paths ../data/\"$PROBLEM_TYPE\"/train/\"$TRAIN_PROBLEM_NAME\" \\\n",
        "    --eval-problems-paths ../data/\"$PROBLEM_TYPE\"/val/\"$VAL_PROBLEM_NAME\" \\\n",
        "    --lr 0.00002 \\\n",
        "    --bsize 64 \\\n",
        "    --buffer-size 20000 \\\n",
        "    --eps-init 1.0 \\\n",
        "    --eps-final 0.01 \\\n",
        "    --gamma 0.99 \\\n",
        "    --eps-decay-steps 30000 \\\n",
        "    --batch-updates 50000 \\\n",
        "    --history-len 1 \\\n",
        "    --init-exploration-steps 5000 \\\n",
        "    --step-freq 4 \\\n",
        "    --target-update-freq 10 \\\n",
        "    --loss mse \\\n",
        "    --opt adam \\\n",
        "    --save-freq 500 \\\n",
        "    --grad_clip 0.1 \\\n",
        "    --grad_clip_norm_type 2 \\\n",
        "    --eval-freq 1000 \\\n",
        "    --eval-time-limit 3600 \\\n",
        "    --core-steps 4 \\\n",
        "    --expert-exploration-prob 0.0 \\\n",
        "    --priority_alpha 0.5 \\\n",
        "    --priority_beta 0.5 \\\n",
        "    --e2v-aggregator sum \\\n",
        "    --n_hidden 1 \\\n",
        "    --hidden_size 64 \\\n",
        "    --decoder_v_out_size 32 \\\n",
        "    --decoder_e_out_size 1 \\\n",
        "    --decoder_g_out_size 1 \\\n",
        "    --encoder_v_out_size 32 \\\n",
        "    --encoder_e_out_size 32 \\\n",
        "    --encoder_g_out_size 32 \\\n",
        "    --core_v_out_size 64 \\\n",
        "    --core_e_out_size 64 \\\n",
        "    --core_g_out_size 32 \\\n",
        "    --activation relu \\\n",
        "    --penalty_size 0.1 \\\n",
        "    --train_time_max_decisions_allowed 500 \\\n",
        "    --test_time_max_decisions_allowed 500 \\\n",
        "    --no_max_cap_fill_buffer \\\n",
        "    --lr_scheduler_gamma 1 \\\n",
        "    --lr_scheduler_frequency 3000 \\\n",
        "    --independent_block_layers 0"
      ],
      "id": "fMwwOBfcjCDN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MD6DHwwjCDN"
      },
      "source": [
        "### Add metadata for evaluation (test set)"
      ],
      "id": "1MD6DHwwjCDN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imKKT3snjCDO"
      },
      "source": [
        "for PROBLEM_NAME in datasets[PROBLEM_TYPE]['inner_test']:\n",
        "  !python add_metadata.py --eval-problems-paths ../data/\"$PROBLEM_TYPE\"/test/\"$PROBLEM_NAME\"\n",
        "\n",
        "for PROBLEM_NAME in datasets[PROBLEM_TYPE]['test']:\n",
        "  !python add_metadata.py --eval-problems-paths ../data/\"$PROBLEM_TYPE\"/\"$PROBLEM_NAME\""
      ],
      "id": "imKKT3snjCDO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am0iALxRjCDO"
      },
      "source": [
        "### Evaluate"
      ],
      "id": "am0iALxRjCDO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1GmjNk3jCDO"
      },
      "source": [
        "MODEL_DIR='Dec08_08-39-57_e63e47f25457'\n",
        "CHECKPOINT='model_50000'"
      ],
      "id": "t1GmjNk3jCDO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0bYPPEdbAkZ"
      },
      "source": [
        "We test this trained model on the inner test set."
      ],
      "id": "v0bYPPEdbAkZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdSllBM8bAlc"
      },
      "source": [
        "for PROBLEM_NAME in datasets[PROBLEM_TYPE]['inner_test']:\n",
        "  for MODEL_DECISION in [10, 50, 100, 300, 500, 1000]:\n",
        "    !python evaluate.py \\\n",
        "      --logdir log \\\n",
        "      --env-name sat-v0 \\\n",
        "      --core-steps -1 \\\n",
        "      --eps-final 0.0 \\\n",
        "      --eval-time-limit 100000000000000 \\\n",
        "      --no_restarts \\\n",
        "      --test_time_max_decisions_allowed \"$MODEL_DECISION\" \\\n",
        "      --eval-problems-paths ../data/\"$PROBLEM_TYPE\"/test/\"$PROBLEM_NAME\" \\\n",
        "      --model-dir runs/\"$MODEL_DIR\" \\\n",
        "      --model-checkpoint \"$CHECKPOINT\".chkp \\\n",
        "      >> runs/\"$MODEL_DIR\"/\"$PROBLEM_NAME\"-graphqsat-max\"$MODEL_DECISION\".tsv"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "AdSllBM8bAlc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMW85WsMbAlf"
      },
      "source": [
        "We test the trained model on the outer test sets."
      ],
      "id": "zMW85WsMbAlf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YQTwDwvjCDO"
      },
      "source": [
        "for PROBLEM_NAME in datasets[PROBLEM_TYPE]['test']:\n",
        "  for MODEL_DECISION in [10, 50, 100, 300, 500, 1000]:\n",
        "    !python evaluate.py \\\n",
        "      --logdir log \\\n",
        "      --env-name sat-v0 \\\n",
        "      --core-steps -1 \\\n",
        "      --eps-final 0.0 \\\n",
        "      --eval-time-limit 100000000000000 \\\n",
        "      --no_restarts \\\n",
        "      --test_time_max_decisions_allowed \"$MODEL_DECISION\" \\\n",
        "      --eval-problems-paths ../data/\"$PROBLEM_TYPE\"/\"$PROBLEM_NAME\" \\\n",
        "      --model-dir runs/\"$MODEL_DIR\" \\\n",
        "      --model-checkpoint \"$CHECKPOINT\".chkp \\\n",
        "      >> runs/\"$MODEL_DIR\"/\"$PROBLEM_NAME\"-graphqsat-max\"$MODEL_DECISION\".tsv"
      ],
      "id": "7YQTwDwvjCDO",
      "execution_count": null,
      "outputs": []
    }
  ]
}